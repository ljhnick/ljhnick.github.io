<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Jiahao Nick LI</title> <meta name="author" content="Jiahao Nick LI"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/jia.jpg?4116e159a22535526950cdef0af9b379"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://ljhnick.github.io/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?e74e74bf055e5729d44a7d031a5ca6a5" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?6185d15ea1982787ad7f435576553d64"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">about<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/assets/pdf/resume.pdf">cv</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title"> <span class="font-weight-bold">Jiahao</span> Nick LI ÊùéÂòâÊòä </h1> <p class="desc"></p> </header> <article> <div class="profile float-right"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/profile_picture-480.webp 480w, /assets/img/profile_picture-800.webp 800w, /assets/img/profile_picture-1400.webp 1400w, " sizes="(min-width: 800px) 231.0px, (min-width: 576px) 30vw, 95vw" type="image/webp"></source> <img src="/assets/img/profile_picture.jpeg?1cf4f4ffc7c19c7096e8a168edff1784" class="img-fluid z-depth-1 rounded" width="100%" height="auto" alt="profile_picture.jpeg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="more-info"> <p> <i class="far fa-envelope"></i> ljhnick@ucla.edu </p> <p> <a class="meta" href="https://scholar.google.com/citations?user=NktGUFUAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank"><i class="fas fa-user-graduate"></i> Google Scholar</a> </p> <p> <a class="meta" href="assets/pdf/resume.pdf"><i class="far fa-file"></i> Curriculum Vitae </a> </p> <p> </p> </div> </div> <div class="clearfix"> <p>I am a researcher in HCI and a software engineer at <a href="https://machinelearning.apple.com/research" rel="external nofollow noopener" target="_blank">Apple Ô£ø</a>. </p> <p>My research area lies in <strong>Human-AI Interaction</strong>, focusing on <strong>Wearable AI Assistance</strong>. Specifically, I build interactive systems to provide proactive assistance for individual users including (1) action prediction, (2) personal memory augmentation and (3) context-aware reminders. </p> <p>I earned my Ph.D. from UCLA in HCI under the supervision of Prof. <a href="https://hci.prof/" rel="external nofollow noopener" target="_blank">Xiang ‚ÄòAnthony‚Äô Chen</a> and B.E. from <a href="https://en.sjtu.edu.cn/" rel="external nofollow noopener" target="_blank">Shanghai Jiao Tong University</a>.</p> <p>I interned at <a href="https://about.facebook.com/realitylabs/" rel="external nofollow noopener" target="_blank">Meta Reality Lab</a>, <a href="https://research.adobe.com/" rel="external nofollow noopener" target="_blank">Adobe Research</a> and <a href="https://www.parc.com/" rel="external nofollow noopener" target="_blank">Palo Alto Research Center (PARC)</a>. I was also a visiting Ph.D. student at <a href="https://www.u-tokyo.ac.jp/en/" rel="external nofollow noopener" target="_blank">University of Tokyo</a> working with <a href="https://www-ui.is.s.u-tokyo.ac.jp/~takeo/index.html" rel="external nofollow noopener" target="_blank">Dr. Takeo Igarashi</a>. </p> <p>üì¢ <strong>I am actively seeking a full-time industrial position.</strong> If you know of any opportunity, please feel free to reach out!</p> </div> <h2><a href="/news/" style="color: inherit;">news</a></h2> <div class="news"> <div class="table-responsive" style="max-height: 60vw"> <table class="table table-sm table-borderless"> <tr> <th scope="row" style="width: 20%">Sep 12, 2024</th> <td> Check out the <a href="https://arxiv.org/abs/2409.08250" rel="external nofollow noopener" target="_blank">preprint</a> of <strong>OmniQuery</strong> (<a href="https://jiahaoli.net/omniquery/" rel="external nofollow noopener" target="_blank">Project website</a>)! OmniQuery enables answering complex questions on your personal captured memories, such as ‚Äú<em>What parties did I attend during CHI 2024?</em>‚Äù. </td> </tr> <tr> <th scope="row" style="width: 20%">Aug 2, 2024</th> <td> One poster accepted to <a href="https://uist.acm.org/2024/" rel="external nofollow noopener" target="_blank">UIST 2024</a> (<strong>OmniQuery</strong>)! Invited as a Proceedings Co-chair for UIST 2025. </td> </tr> <tr> <th scope="row" style="width: 20%">Apr 13, 2024</th> <td> Invited as a PC member and Proceedings Co-chair for <a href="https://uist.acm.org/2024/" rel="external nofollow noopener" target="_blank">UIST 2024</a>! </td> </tr> <tr> <th scope="row" style="width: 20%">Jan 19, 2024</th> <td> Two papers (<strong>OmniActions</strong> &amp; <strong>Human I/O</strong>) got accepted by <a href="https://chi2024.acm.org/" rel="external nofollow noopener" target="_blank">CHI 2024</a> <img class="emoji" title=":tada:" alt=":tada:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f389.png" height="20" width="20">. <a href="https://jiahaoli.net/assets/pdf/CHI_2024_OmniActions_Camera_Ready.pdf" rel="external nofollow noopener" target="_blank">OmniActions</a> is from my Meta internship on user intent prediction in pervasive AR scenarios leveraging LLMs and VLMs. <a href="https://liubruce.me/assets/pdf/liu2024humanio.pdf" rel="external nofollow noopener" target="_blank">Human I/O</a> is on identifying situational impairments using egocentric videos leveraging reasonings of LLMs. </td> </tr> </table> </div> </div> <h2><a href="/publications/" style="color: inherit;">selected publications</a></h2> <div class="publications"> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-3 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/publication_preview/omniquery_poster_preview-480.webp 480w, /assets/img/publication_preview/omniquery_poster_preview-800.webp 800w, /assets/img/publication_preview/omniquery_poster_preview-1400.webp 1400w, " sizes="250px" type="image/webp"></source> <img src="/assets/img/publication_preview/omniquery_poster_preview.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="omniquery_poster_preview.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="omniquery" class="col-sm-9"> <div class="title">OmniQuery: Contextually Augmenting Captured Multimodal Memories to Enable Personal Question Answering.</div> <div class="author"> <em>Jiahao Nick Li</em>,¬†Zhuohao (Jerry) Zhang,¬†and¬†Jiaju Ma</div> <div class="periodical"> <em>In Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems (CHI ‚Äô25) (To Appear)</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/omniquery_arxiv.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="http://arxiv.org/abs/https://arxiv.org/abs/2409.08250" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://youtu.be/U9PIuuoMMFs" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="/assets/pdf/omniquery_poster.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a> <a href="omniquery" class="btn btn-sm z-depth-0" role="button">Website</a> </div> <div class="abstract hidden"> <div style="text-align: center;"> <figure> <iframe src="https://youtu.be/U9PIuuoMMFs" class="img-fluid rounded z-depth-1" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen width="auto" height="auto"></iframe> </figure> </div> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/publication_preview/omniactions-480.webp 480w, /assets/img/publication_preview/omniactions-800.webp 800w, /assets/img/publication_preview/omniactions-1400.webp 1400w, " sizes="250px" type="image/webp"></source> <img src="/assets/img/publication_preview/omniactions.gif" class="preview z-depth-1 rounded" width="100%" height="auto" alt="omniactions.gif" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="omniactions" class="col-sm-9"> <div class="title">OmniActions: Predicting Digital Actions in Response to Real-World Multimodal Sensory Inputs with LLMs</div> <div class="author"> <em>Jiahao Nick Li</em>,¬†Yan Xu,¬†Tovi Grossman, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Stephanie Santosa, Michelle Li' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em>In Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems (CHI ‚Äô24)</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/CHI_2024_OmniActions_Camera_Ready.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="http://arxiv.org/abs/https://arxiv.org/abs/2405.03901" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://dl.acm.org/doi/10.1145/3613904.3642068" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/publication_preview/humanio_thumbnail-480.webp 480w, /assets/img/publication_preview/humanio_thumbnail-800.webp 800w, /assets/img/publication_preview/humanio_thumbnail-1400.webp 1400w, " sizes="250px" type="image/webp"></source> <img src="/assets/img/publication_preview/humanio_thumbnail.gif" class="preview z-depth-1 rounded" width="100%" height="auto" alt="humanio_thumbnail.gif" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="humanio" class="col-sm-9"> <div class="title">Human I/O: Towards a Unified Approach to Detecting Situational Impairments in Everyday Activities</div> <div class="author"> Xingyu Bruce Liu,¬†<em>Jiahao Nick Li</em>,¬†David Kim, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Xiang Anthony Chen, Ruofei Du' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em>In Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems (CHI ‚Äô24)</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <strong> <i class="fas fa-medal"></i> Best Paper Honorable Mention </strong> <br> <a href="/assets/pdf/liu2024humanio.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://liubruce.me/human_io/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> <a href="10.1145/3613904.3642065" class="btn btn-sm z-depth-0" role="button">DOI</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/publication_preview/roman-480.webp 480w, /assets/img/publication_preview/roman-800.webp 800w, /assets/img/publication_preview/roman-1400.webp 1400w, " sizes="250px" type="image/webp"></source> <img src="/assets/img/publication_preview/roman.gif" class="preview z-depth-1 rounded" width="100%" height="auto" alt="roman.gif" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="10.1145/3491102.3501818" class="col-sm-9"> <div class="title">Roman: Making Everyday Objects Robotically Manipulable with 3D-Printable Add-on Mechanisms</div> <div class="author"> <em>Jiahao Li</em>,¬†Alexis Samoylov,¬†Jeeeun Kim, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Xiang ‚ÄôAnthony‚Äô Chen' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>In Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems (CHI ‚Äô22)</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="/assets/pdf/roman.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="10.1145/3491102.3501818" class="btn btn-sm z-depth-0" role="button">DOI</a> </div> <div class="abstract hidden"> <p>One important vision of robotics is to provide physical assistance by manipulating different everyday objects, e.g., hand tools, kitchen utensils. However, many objects designed for dexterous hand-control are not easily manipulable by a single robotic arm with a generic parallel gripper. Complementary to existing research on developing grippers and control algorithms, we present Roman, a suite of hardware design and software tool support for robotic engineers to create 3D printable mechanisms attached to everyday handheld objects, making them easier to be manipulated by conventional robotic arms. The Roman hardware comes with a versatile magnetic gripper that can snap on/off handheld objects and drive add-on mechanisms to perform tasks. Roman also provides software support to register and author control programs. To validate our approach, we designed and fabricated Roman mechanisms for 14 everyday objects/tasks presented within a design space and conducted expert interviews with robotic engineers indicating that Roman serves as a practical alternative for enabling robotic manipulation of everyday objects.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/publication_preview/romeo-480.webp 480w, /assets/img/publication_preview/romeo-800.webp 800w, /assets/img/publication_preview/romeo-1400.webp 1400w, " sizes="250px" type="image/webp"></source> <img src="/assets/img/publication_preview/romeo.gif" class="preview z-depth-1 rounded" width="100%" height="auto" alt="romeo.gif" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="10.1145/3379337.3415826" class="col-sm-9"> <div class="title">Romeo: A Design Tool for Embedding Transformable Parts in 3D Models to Robotically Augment Default Functionalities</div> <div class="author"> <em>Jiahao Li</em>,¬†Meilin Cui,¬†Jeeeun Kim, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Xiang ‚ÄôAnthony‚Äô Chen' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>In Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technolog (UIST ‚Äô20)</em>, 2020 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="/assets/pdf/romeo.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="10.1145/3379337.3415826" class="btn btn-sm z-depth-0" role="button">DOI</a> </div> <div class="abstract hidden"> <p>Reconfiguring shapes of objects enables transforming existing passive objects with robotic functionalities, e.g., a transformable coffee cup holder can be attached to a chair‚Äôs armrest, a piggy bank can reach out an arm to ‚Äôsteal‚Äô coins. Despite the advance in end-user 3D design and fabrication, it remains challenging for non-experts to create such ‚Äôtransformables‚Äô using existing tools due to the requirement of specific engineering knowledge such as mechanisms and robotic design. We present Romeo ‚Äì a design tool for creating transformables to robotically augment objects‚Äô default functionalities. Romeo allows users to transform an object into a robotic arm by expressing at a high level what type of task is expected. Users can select which part of the object to be transformed, specify motion points in space for the transformed part to follow and the corresponding action to be taken. Romeo then automatically generates a robotic arm embedded in the transformable part ready for fabrication. A design session validated this tool where participants used Romeo to accomplish controlled design tasks and to open-endedly create coin-stealing piggy banks by transforming 3D objects of their own choice.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/publication_preview/robiot-480.webp 480w, /assets/img/publication_preview/robiot-800.webp 800w, /assets/img/publication_preview/robiot-1400.webp 1400w, " sizes="250px" type="image/webp"></source> <img src="/assets/img/publication_preview/robiot.gif" class="preview z-depth-1 rounded" width="100%" height="auto" alt="robiot.gif" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="10.1145/3332165.3347894" class="col-sm-9"> <div class="title">Robiot: A Design Tool for Actuating Everyday Objects with Automatically Generated 3D Printable Mechanisms</div> <div class="author"> <em>Jiahao Li</em>,¬†Jeeeun Kim,¬†and¬†Xiang ‚ÄôAnthony‚Äô Chen</div> <div class="periodical"> <em>In Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology (UIST ‚Äô19)</em>, 2019 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="/assets/pdf/robiot.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="10.1145/3332165.3347894" class="btn btn-sm z-depth-0" role="button">DOI</a> </div> <div class="abstract hidden"> <p>Users can now easily communicate digital information with an Internet of Things; in contrast, there remains a lack of support to automate physical tasks that involve legacy static objects, e.g. adjusting a desk lamp‚Äôs angle for optimal brightness, turning on/off a manual faucet when washing dishes, sliding a window to maintain a preferred indoor temperature. Automating these simple physical tasks has the potential to improve people‚Äôs quality of life, which is particularly important for people with a disability or in situational impairment.We present Robiot ‚Äì a design tool for generating mechanisms that can be attached to, motorized, and actuating legacy static objects to perform simple physical tasks. Users only need to take a short video manipulating an object to demonstrate an intended physical behavior. Robiot then extracts requisite parameters and automatically generates 3D models of the enabling actuation mechanisms by performing a scene and motion analysis of the 2D video in alignment with the object‚Äôs 3D model. In an hour-long design session, six participants used Robiot to actuate seven everyday objects, imbuing them with the robotic capability to automate various physical tasks.</p> </div> </div> </div> </li> </ol> </div> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> ¬© Copyright 2025 Jiahao Nick LI. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?9b43d6e67ddc7c0855b1478ee4c48c2d" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>