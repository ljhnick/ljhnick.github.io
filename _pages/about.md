---
layout: about
title: about
permalink: /
subtitle: 

profile:
  align: right
  image: profile_picture.jpeg
  image_circular: false # crops the image to make it circular
  more_info: >
    <p>
    <i class="far fa-envelope"></i> ljhnick@ucla.edu
    </p>
    <p>
    <a class="meta" href="https://scholar.google.com/citations?user=NktGUFUAAAAJ&hl=en"><i class="fas fa-user-graduate"></i> Google Scholar</a>
    </p>
    <p>
    <a class="meta" href="assets/pdf/resume.pdf"><i class="far fa-file"></i> Curriculum Vitae </a>
    </p>
    <p>
    </p>

news: true  # includes a list of news items
latest_posts: false  # includes a list of the newest posts
selected_papers: true # includes a list of papers marked as "selected={true}"
social: false  # includes social icons at the bottom of the page
---

I am a researcher in HCI and a software engineer at [Apple ï£¿](https://machinelearning.apple.com/research).
<!-- I am a Ph.D. candidate at [UCLA HCI Research](https://hci.ucla.edu/) working with Prof. [Xiang 'Anthony' Chen](https://hci.prof/). -->

My research area lies in **Human-AI Interaction**, focusing on **Wearable AI Assistance**. 
Specifically, I build interactive systems to provide proactive assistance for individual users including (1) action prediction, (2) personal memory augmentation and (3) context-aware reminders.
<!-- Such systems are powered by cutting-edge AI models (e.g., LLMs, VLMs) and are enhanced by incorporating:
(1) external and task-specific knowledge, sourced through both crowdsourcing and targeted data collection, and
 <!-- via crowdsourcing and data collection, and -->
<!-- (2) the retrieval of long-form multimodal memories, facilitating contextual understanding for users.
Recently, I have been working on building a multimodal retrieval augmented pipeline to address the natural language query (NLQ) task on the egocentric video datasets (e.g., Ego4D). -->
<!-- long-form multimodal memories, which can be retrieved to enhance the contextual understanding of users.
 retrieval for enhanced semantically and episodically memory retrieval. 
design algorithms and build pipelines to augment existing AI models (e.g., LLMs, VLMs) with external knowledge   -->

<!-- Augment multimodal AI models with external and domain-specific knowledge with crowdsourcing and data collection
Augment with long-form multimodal memories to effectively retrieve semantically and episodically related memories for personalized assistance.  -->

<!-- My research interest lies in **Egocentric Contextual AI for Pervasive AR devices**,  -->

<!-- My research area focuses on human-AI interaction with a focus on building 
 **designing studies to understand user needs** and **building AI-powered interactive systems** for novel real-world tasks. I am particularly interested in AI-assisted tasks using personal egocentric data.
This includes: (1) embodied AI assistance for pervasive AR, and (2) long-form egocentric video understanding.  -->
<!-- I build systems to augment human capabilities in everyday activities, which include: (1) predicting users' intent in interaction with egocentric data
and (2) providing proactive assistance based on current context and historical memory. -->

I earned my Ph.D. from UCLA in HCI under the supervision of Prof. [Xiang 'Anthony' Chen](https://hci.prof/) and B.E. from [Shanghai Jiao Tong University](https://en.sjtu.edu.cn/).


I interned at [Meta Reality Lab](https://about.facebook.com/realitylabs/), [Adobe Research](https://research.adobe.com/) and [Palo Alto Research Center (PARC)](https://www.parc.com/). I was also a visiting Ph.D. student at [University of Tokyo](https://www.u-tokyo.ac.jp/en/) working with [Dr. Takeo Igarashi](https://www-ui.is.s.u-tokyo.ac.jp/~takeo/index.html).
<!-- Prior to that, I received my Bachelor of Engineering from [Shanghai Jiao Tong University](https://en.sjtu.edu.cn/). -->

